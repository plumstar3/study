# run.py (è«–æ–‡ã®è©•ä¾¡æ–¹æ³•ã‚’å®Œå…¨ã«å†ç¾ã—ãŸæœ€çµ‚ç‰ˆ)

import os
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import layers, models, optimizers, losses, callbacks
from sklearn.metrics import mean_squared_error
from scipy.stats import pearsonr
from sklearn.preprocessing import StandardScaler

# ==============================================================================
# 1. ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼é–¢é€£
# ==============================================================================
def load_data(path='./Data/soybean_data.csv'):
    """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸€åº¦ã ã‘èª­ã¿è¾¼ã‚€ã€‚"""
    if not os.path.exists(path):
        print(f"ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ« '{path}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return None
    df = pd.read_csv(path)
    df = df[df['yield'] >= 5].reset_index(drop=True)
    return df

class SoybeanDataGenerator(tf.keras.utils.Sequence):
    """è«–æ–‡ã®ã‚³ãƒ³ã‚»ãƒ—ãƒˆã«åŸºã¥ãã€åœ°åŸŸã”ã¨ã®5å¹´ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ç”Ÿæˆã™ã‚‹ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿"""
    def __init__(self, df, batch_size, shuffle=True):
        self.df = df.copy()
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.feature_cols = self.df.columns.drop(['loc_ID', 'year', 'yield'])
        
        self.loc_year_dict = { (row.loc_ID, int(row.year)): row for _, row in self.df.iterrows() }
        
        self.sequences = []
        loc_ids = self.df['loc_ID'].unique()
        all_years = sorted(self.df['year'].unique())

        target_years = sorted(self.df[self.df['year'] >= self.df['year'].min() + 4]['year'].unique())

        for loc_id in loc_ids:
            for target_year in target_years:
                seq_years = list(range(target_year - 4, target_year + 1))
                if all((loc_id, year) in self.loc_year_dict for year in seq_years):
                    self.sequences.append({'loc_id': loc_id, 'years': seq_years})
        
        self.indices = np.arange(len(self.sequences))
        self.on_epoch_end()

    def __len__(self):
        return int(np.ceil(len(self.sequences) / self.batch_size))

    def __getitem__(self, index):
        batch_indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]
        batch_seq_info = [self.sequences[i] for i in batch_indices]
        actual_batch_size = len(batch_seq_info)

        X_dict = {
            'e_input': np.zeros((actual_batch_size, 5, 312)),
            's_input': np.zeros((actual_batch_size, 5, 66)),
            'p_input': np.zeros((actual_batch_size, 5, 14)),
        }
        Y = np.zeros((actual_batch_size, 1))

        for i, seq_info in enumerate(batch_seq_info):
            loc_id, years = seq_info['loc_id'], seq_info['years']
            for j, year in enumerate(years):
                sample = self.loc_year_dict[(loc_id, year)]
                features = sample[self.feature_cols].values
                X_dict['e_input'][i, j, :] = features[0:312]
                X_dict['s_input'][i, j, :] = features[312:378]
                X_dict['p_input'][i, j, :] = features[378:392]
            Y[i] = self.loc_year_dict[(loc_id, years[-1])]['yield']
        return X_dict, Y

    def on_epoch_end(self):
        if self.shuffle:
            np.random.shuffle(self.indices)

# ==============================================================================
# 2. ãƒ¢ãƒ‡ãƒ«å®šç¾©
# ==============================================================================
def create_cnn_block(input_layer, filters, kernel_sizes, name=""):
    x = input_layer
    for i, (f, k) in enumerate(zip(filters, kernel_sizes)):
        x = layers.Conv1D(f, k, activation='relu', padding='same', name=f"{name}_conv_{i}")(x)
        x = layers.MaxPooling1D(pool_size=2, strides=1, padding='same', name=f"{name}_pool_{i}")(x)
    return layers.Flatten(name=f"{name}_flatten")(x)

def build_and_compile_model(sequence_length=5):
    e_input = layers.Input(shape=(sequence_length, 312), name="e_input")
    s_input = layers.Input(shape=(sequence_length, 66), name="s_input")
    p_input = layers.Input(shape=(sequence_length, 14), name="p_input")

    e_proc_input = layers.Input(shape=(312,), name="e_proc_input")
    e_reshaped = layers.Reshape((6, 52))(e_proc_input)
    shared_cnn_input = layers.Input(shape=(52, 1), name="shared_cnn_input")
    cnn_block_output = create_cnn_block(shared_cnn_input, [8, 16], [3, 3], name="shared_e_cnn")
    shared_e_cnn = models.Model(inputs=shared_cnn_input, outputs=cnn_block_output, name="Shared_E_CNN")
    e_cnn_outs = [shared_e_cnn(e_reshaped[:, i, :, None]) for i in range(6)]
    e_cnn_model = models.Model(inputs=e_proc_input, outputs=layers.Concatenate()(e_cnn_outs), name="E_CNN_Model")
    
    s_proc_input = layers.Input(shape=(66,), name="s_proc_input")
    s_reshaped = layers.Reshape((6, 11))(s_proc_input)
    s_cnn_out = create_cnn_block(s_reshaped, [16, 32], [3, 3], name="s_cnn")
    s_cnn_model = models.Model(inputs=s_proc_input, outputs=s_cnn_out, name="S_CNN_Model")

    e_processed = layers.TimeDistributed(e_cnn_model)(e_input)
    s_processed = layers.TimeDistributed(s_cnn_model)(s_input)
    p_processed = layers.TimeDistributed(layers.Flatten())(p_input)
    
    merged = layers.Concatenate()([e_processed, s_processed, p_processed])
    x = layers.Dense(128, activation='relu')(merged)
    x = layers.LSTM(64, return_sequences=False, dropout=0.2)(x)
    output = layers.Dense(1, name="Yhat1")(x)

    model = models.Model(inputs=[e_input, s_input, p_input], outputs=output)
    model.compile(optimizer=optimizers.Adam(learning_rate=0.0003), loss=losses.Huber(), metrics=['mae'])
    return model

# ==============================================================================
# 3. è¨“ç·´ã¨è©•ä¾¡ã®ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°
# ==============================================================================
def main():
    print("ğŸŒ± å¤§è±†åé‡äºˆæ¸¬ãƒ¢ãƒ‡ãƒ« - è«–æ–‡ã®è©•ä¾¡æ–¹æ³•ã‚’å†ç¾")
    data_path = './Data/soybean_data.csv'
    
    full_df = load_data(path=data_path)
    if full_df is None: return

    test_years = [2016, 2017, 2018]
    results = []
    
    for test_year in test_years:
        print("\n" + "="*50)
        print(f"ğŸ”¬ ãƒ†ã‚¹ãƒˆå¹´: {test_year} ã§ã®è©•ä¾¡ã‚’é–‹å§‹")
        print("="*50)

        # 2. ãƒ‡ãƒ¼ã‚¿åˆ†å‰² (è«–æ–‡ã®è¨˜è¿°ã«å³å¯†ã«å¾“ã†)
        train_df_unscaled = full_df[full_df['year'] < test_year].copy()
        test_df_unscaled = full_df[full_df['year'] == test_year].copy()
        
        if train_df_unscaled.empty or test_df_unscaled.empty:
            print("ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
            continue

        # 3. æ¨™æº–åŒ– (è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®æƒ…å ±ã®ã¿ã§å­¦ç¿’)
        feature_cols = full_df.columns.drop(['loc_ID', 'year', 'yield'])
        scaler = StandardScaler()
        train_df = train_df_unscaled.copy(); train_df[feature_cols] = scaler.fit_transform(train_df_unscaled[feature_cols])
        test_df = test_df_unscaled.copy(); test_df[feature_cols] = scaler.transform(test_df_unscaled[feature_cols])
        
        # 4. ãƒ‡ãƒ¼ã‚¿ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ã®ä½œæˆ
        print("\n--- ãƒ‡ãƒ¼ã‚¿æº–å‚™ ---")
        train_generator = SoybeanDataGenerator(train_df, batch_size=64)
        test_generator = SoybeanDataGenerator(pd.concat([train_df, test_df]), batch_size=64, shuffle=False)
        # è©•ä¾¡å¯¾è±¡ã¯ãƒ†ã‚¹ãƒˆå¹´ã®ã¿ã«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        test_generator.sequences = [s for s in test_generator.sequences if s['years'][-1] == test_year]
        test_generator.indices = np.arange(len(test_generator.sequences))

        print(f"è¨“ç·´ç”¨ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æ•°: {len(train_generator.sequences)}")
        print(f"ãƒ†ã‚¹ãƒˆç”¨ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æ•° ({test_year}å¹´): {len(test_generator.sequences)}")

        if len(train_generator) == 0 or len(test_generator) == 0:
            print("ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
            continue
            
        # 5. ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰ã¨è¨“ç·´
        model = build_and_compile_model()
        
        print("\n--- ãƒ¢ãƒ‡ãƒ«è¨“ç·´é–‹å§‹ ---")
        # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã¯ä½¿ã‚ãšã€è¨“ç·´ãƒ‡ãƒ¼ã‚¿å…¨ä½“ã§å­¦ç¿’
        model.fit(train_generator, epochs=100, verbose=2,
                  callbacks=[callbacks.EarlyStopping(monitor='loss', patience=15)])
        print(f"âœ… {test_year}å¹´ã‚’ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´å®Œäº†")
        
        # 6. è©•ä¾¡
        print("\n--- ãƒ¢ãƒ‡ãƒ«è©•ä¾¡é–‹å§‹ ---")
        Y_pred = model.predict(test_generator).flatten()
        
        # æ­£è§£ãƒ©ãƒ™ãƒ«ã‚’ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æƒ…å ±ã‹ã‚‰ç›´æ¥å–å¾—
        true_yields = []
        for seq_info in test_generator.sequences:
            true_yields.append(test_generator.loc_year_dict[(seq_info['loc_id'], seq_info['years'][-1])]['yield'])
        Y_true = np.array(true_yields)

        rmse = np.sqrt(mean_squared_error(Y_true, Y_pred))
        results.append({'year': test_year, 'rmse': rmse})
        print(f"ğŸ“Š {test_year}å¹´ã®Test RMSE: {rmse:.4f}")

    # 7. æœ€çµ‚çµæœã®è¡¨ç¤º
    print("\n" + "="*50)
    print("ğŸ‰ å…¨ã¦ã®ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³è©•ä¾¡ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
    print("="*50)
    
    if results:
        results_df = pd.DataFrame(results).set_index('year')
        print(results_df)
        avg_rmse = results_df['rmse'].mean()
        print(f"\n  => å¹³å‡RMSE ({min(test_years)}-{max(test_years)}): {avg_rmse:.4f}")
    else:
        print("è©•ä¾¡ã‚’å®Ÿè¡Œã§ãã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

if __name__ == "__main__":
    main()