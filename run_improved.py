# -*- coding: utf-8 -*-
import os
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import layers, models, optimizers, losses, callbacks
from sklearn.metrics import mean_squared_error
from scipy.stats import pearsonr
from tensorflow.keras.layers import Bidirectional


# 1. ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼é–¢é€£
def load_and_preprocess_data(path='./Data/soybean_data.csv'):
    """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€åŸºæœ¬çš„ãªå‰å‡¦ç†ï¼ˆæ¨™æº–åŒ–ãªã©ï¼‰ã‚’è¡Œã†ã€‚"""
    if not os.path.exists(path):
        print(f"ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ« '{path}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
        return None
    df = pd.read_csv(path)
    
    feature_cols = df.columns[3:]
    train_df = df[df['year'] <= 2017]
    mean = train_df[feature_cols].mean()
    std = train_df[feature_cols].std()
    std[std == 0] = 1.0
    df[feature_cols] = (df[feature_cols] - mean) / std
    df = df.fillna(0)
    df = df[df['yield'] >= 5].reset_index(drop=True)
    return df

def create_year_loc_dict_and_avg(df):
    """å¹´ã¨åœ°åŸŸ(loc_ID)ã‚’ã‚­ãƒ¼ã«ã—ãŸãƒ‡ãƒ¼ã‚¿è¾æ›¸ã¨ã€å¹´ã”ã¨ã®å¹³å‡åé‡è¾æ›¸ã‚’ä½œæˆã™ã‚‹ã€‚"""
    loc_year_dict = { (row.loc_ID, int(row.year)): row for index, row in df.iterrows() }
    avg_yield_by_year = df.groupby('year')['yield'].mean()
    mean_yield = avg_yield_by_year.mean()
    std_yield = avg_yield_by_year.std()
    avg_dict = (avg_yield_by_year - mean_yield) / std_yield
    
    if 2018 not in avg_dict.index and 2017 in avg_dict.index:
        avg_dict[2018] = avg_dict.get(2017, 0)
        
    return loc_year_dict, {str(k): v for k, v in avg_dict.to_dict().items()}

class SoybeanDataGenerator(tf.keras.utils.Sequence):
    """Kerasãƒ¢ãƒ‡ãƒ«ã®ãŸã‚ã®ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ï¼ˆåœ°åŸŸè€ƒæ…®ãƒ»TimeDistributedå¯¾å¿œç‰ˆï¼‰"""
    def __init__(self, df, loc_year_dict, avg_dict, batch_size, is_training=True, target_year=None):
        self.loc_year_dict = loc_year_dict
        self.avg_dict = avg_dict
        self.batch_size = batch_size
        self.target_year = target_year
        self.is_training = is_training
        
        self.sequences = []
        loc_ids = df['loc_ID'].unique()
        all_years = sorted(df['year'].unique())

        for loc_id in loc_ids:
            for i in range(len(all_years) - 4):
                seq_years = all_years[i:i+5]
                if all((loc_id, year) in self.loc_year_dict for year in seq_years):
                    self.sequences.append({'loc_id': loc_id, 'years': seq_years})

        # --- å¯¾è±¡å¹´ã‚’å«ã‚€ or å«ã¾ãªã„ ã§ãƒ•ã‚£ãƒ«ã‚¿ ---
        if target_year is not None:
            if is_training:
                self.sequences = [s for s in self.sequences if target_year not in s['years']]
            else:
                self.sequences = [s for s in self.sequences if target_year in s['years']]
        
        print(f"{'è¨“ç·´' if is_training else 'æ¤œè¨¼'}ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãŒã€{len(self.sequences)}å€‹ã®æœ‰åŠ¹ãªã€Œåœ°åŸŸ-5å¹´ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚")
        self.indices = np.arange(len(self.sequences))
        self.on_epoch_end()

    def __len__(self):
        if len(self.sequences) == 0: return 0
        return int(np.floor(len(self.sequences) / self.batch_size))

    def __getitem__(self, index):
        batch_indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]
        batch_seq_info = [self.sequences[i] for i in batch_indices]
        actual_batch_size = len(batch_seq_info)

        #å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’è¾æ›¸å½¢å¼ã§æº–å‚™
        X_dict = {
            'e_input': np.zeros((actual_batch_size, 5, 312)),
            's_input': np.zeros((actual_batch_size, 5, 66)),
            'p_input': np.zeros((actual_batch_size, 5, 14)),
            'ybar_input': np.zeros((actual_batch_size, 5, 1))
        }
        Y_dict = {
            'Yhat1': np.zeros((actual_batch_size, 1)),
            'Yhat2': np.zeros((actual_batch_size, 4, 1))
        }

        for i, seq_info in enumerate(batch_seq_info):
            loc_id = seq_info['loc_id']
            years = seq_info['years']
            
            for j, year in enumerate(years):
                sample = self.loc_year_dict[(loc_id, year)]
                features = sample.iloc[3:].values # ID, year, yieldã‚’é™¤ã
                
                # ç‰¹å¾´é‡ã‚’å„å…¥åŠ›ã«å‰²ã‚Šå½“ã¦
                X_dict['e_input'][i, j, :] = features[0:312]
                X_dict['s_input'][i, j, :] = features[312:378]
                X_dict['p_input'][i, j, :] = features[378:392]
                X_dict['ybar_input'][i, j, 0] = self.avg_dict[str(year)]

            Y_dict['Yhat1'][i] = self.loc_year_dict[(loc_id, years[-1])]['yield']
            past_yields = [self.loc_year_dict[(loc_id, y)]['yield'] for y in years[:-1]]
            Y_dict['Yhat2'][i] = np.array(past_yields).reshape(4, 1)

        return X_dict, Y_dict

    def on_epoch_end(self):
        np.random.shuffle(self.indices)

# 2. ãƒ¢ãƒ‡ãƒ«å®šç¾©
def create_cnn_block(input_layer, filters, kernel_sizes):
    """æ±ç”¨çš„ãªCNNãƒ–ãƒ­ãƒƒã‚¯ã‚’ä½œæˆ"""
    x = input_layer
    for f, k in zip(filters, kernel_sizes):
        x = layers.Conv1D(f, k, activation='relu', padding='same')(x)
        x = layers.MaxPooling1D(pool_size=2, strides=1, padding='same')(x) # ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰ã‚’èª¿æ•´
    return layers.Flatten()(x)

def build_and_compile_model():
    """
    è«–æ–‡ã®è¨­è¨ˆæ€æƒ³ï¼ˆé‡ã¿å…±æœ‰ã¨TimeDistributedï¼‰ã‚’å¿ å®Ÿã«å†ç¾ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã™ã‚‹ã€‚
    """
    # --- å…¥åŠ›å±¤ã®å®šç¾© (å¹´ã”ã¨ã®å¤‰åŒ–ã‚’æ‰ãˆã‚‹ãŸã‚ã€5ã¤ã®ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã‚’æŒã¤) ---
    e_input = layers.Input(shape=(5, 312), name="e_input")
    s_input = layers.Input(shape=(5, 66), name="s_input")
    p_input = layers.Input(shape=(5, 14), name="p_input")
    ybar_input = layers.Input(shape=(5, 1), name="ybar_input")

    # --- ç‰¹å¾´é‡å‡¦ç†ãƒ–ãƒ­ãƒƒã‚¯ã®å®šç¾© (ã‚µãƒ–ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦) ---
    # âœ¨ã€ä¿®æ­£ç‚¹1ã€‘: 6ç¨®é¡ã®æ°—è±¡ãƒ‡ãƒ¼ã‚¿å…¨ã¦ã«é©ç”¨ã™ã‚‹ã€Œå…±æœ‰CNNãƒ–ãƒ­ãƒƒã‚¯ã€ã‚’1ã¤ã ã‘å®šç¾©
    e_cnn_input = layers.Input(shape=(52, 1), name="e_cnn_input")
    x = layers.Conv1D(8, 9, activation='relu', padding='valid')(e_cnn_input)
    x = layers.AveragePooling1D(2)(x)
    x = layers.Conv1D(12, 3, activation='relu', padding='valid')(x)
    x = layers.AveragePooling1D(2)(x)
    e_cnn_output = layers.Flatten()(x)
    shared_e_cnn = models.Model(inputs=e_cnn_input, outputs=e_cnn_output, name="Shared_E_CNN")

    # âœ¨ã€ä¿®æ­£ç‚¹2ã€‘: 6ã¤ã®æ°—è±¡ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã™ã‚‹ãŸã‚ã®ãƒ©ãƒƒãƒ‘ãƒ¼ãƒ¢ãƒ‡ãƒ«ã‚’å®šç¾©
    e_proc_input = layers.Input(shape=(312,), name="e_proc_input")
    e_reshaped = layers.Reshape((6, 52, 1))(e_proc_input)
    # 6ã¤ã®å…¥åŠ›ãã‚Œãã‚Œã«ã€ä¸Šã§å®šç¾©ã—ãŸã€Œå…¨ãåŒã˜å…±æœ‰CNNã€ã‚’é©ç”¨ã™ã‚‹
    e_sub_outputs = [shared_e_cnn(e_reshaped[:, i]) for i in range(6)]
    e_proc_output = layers.Concatenate()(e_sub_outputs)
    e_processor = models.Model(inputs=e_proc_input, outputs=e_proc_output, name="E_Processor")

    # åœŸå£Œ(S)ãƒ‡ãƒ¼ã‚¿ç”¨CNNãƒ¢ãƒ‡ãƒ«
    s_proc_input = layers.Input(shape=(66,), name="s_proc_input")
    s_reshaped = layers.Reshape((6, 11))(s_proc_input)
    s_cnn_out = layers.Flatten()(layers.Conv1D(16, 3, activation='relu')(s_reshaped))
    s_processor = models.Model(inputs=s_proc_input, outputs=s_cnn_out, name="S_Processor")

    # --- TimeDistributedã§å„ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã«ç‰¹å¾´é‡å‡¦ç†ã‚’é©ç”¨ ---
    e_processed = layers.TimeDistributed(e_processor, name="TDD_E_Processor")(e_input)
    s_processed = layers.TimeDistributed(s_processor, name="TDD_S_Processor")(s_input)
    p_processed = layers.TimeDistributed(layers.Flatten(), name="TDD_P_Flatten")(p_input)

    # --- å…¨ã¦ã®ç‰¹å¾´é‡ã‚’çµåˆã—ã€LSTMã«å…¥åŠ› ---
    merged = layers.Concatenate()([e_processed, s_processed, p_processed, ybar_input])
    x = layers.Dense(128, activation='relu')(merged)
    x = layers.LSTM(64, return_sequences=True, dropout=0.2)(x)
    output = layers.TimeDistributed(layers.Dense(1))(x)
    
    Yhat1 = layers.Identity(name='Yhat1')(output[:, -1, :])
    Yhat2 = layers.Identity(name='Yhat2')(output[:, :-1, :])

    model = models.Model(inputs=[e_input, s_input, p_input, ybar_input], outputs=[Yhat1, Yhat2])
    
    model.compile(optimizer=optimizers.Adam(learning_rate=0.0003),
                  loss={'Yhat1': losses.Huber(), 'Yhat2': losses.Huber()},
                  loss_weights={'Yhat1': 1.0, 'Yhat2': 0.0},
                  metrics={'Yhat1': 'mae'})
    return model

# 3. è¨“ç·´ã¨è©•ä¾¡ï¼ˆè«–æ–‡æº–æ‹ : 2016, 2017, 2018å¹´ã‚’æ¤œè¨¼å¹´ã¨ã—ã¦å€‹åˆ¥ã«è©•ä¾¡ï¼‰
def run_training_and_evaluation():
    print("\n[ãƒ¢ãƒ‡ãƒ«è©•ä¾¡] è«–æ–‡æ–¹å¼ï¼ˆ2016ã€œ2018å¹´ã‚’æ¤œè¨¼å¹´ã¨ã™ã‚‹ï¼‰ã§è¨“ç·´ãƒ»è©•ä¾¡ã‚’é–‹å§‹ã—ã¾ã™ã€‚")
    df = load_and_preprocess_data()
    if df is None:
        return
    
    check_years = list(range(2012, 2017))
    valid_locs = []

    for loc_id in df['loc_ID'].unique():
        years = df[df['loc_ID'] == loc_id]['year'].unique()
        if all(y in years for y in check_years):
            valid_locs.append(loc_id)

    print(f"æ¤œè¨¼å¹´2016ã®ãŸã‚ã®æœ‰åŠ¹ãªloc_IDæ•°: {len(valid_locs)}")

    loc_year_dict, avg_dict = create_year_loc_dict_and_avg(df)
    
    results = {}

    for test_year in [2016, 2017, 2018]:
        print(f"\n=== è©•ä¾¡å¯¾è±¡å¹´: {test_year} ===")

        # --- å­¦ç¿’ãƒ»æ¤œè¨¼ç”¨ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®ä½œæˆ ---
        train_df = df[df['year'] < test_year].copy()
        val_df = df[df['year'].between(test_year - 4, test_year)].copy()

        # --- ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ä½œæˆ ---
        train_generator = SoybeanDataGenerator(train_df, loc_year_dict, avg_dict, batch_size=32, is_training=True, target_year=test_year)
        val_generator = SoybeanDataGenerator(val_df, loc_year_dict, avg_dict, batch_size=26, is_training=False, target_year=test_year)

        if len(train_generator) == 0 or len(val_generator) == 0:
            print(f"âš ï¸ å­¦ç¿’ or æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã‚‹ãŸã‚ {test_year}å¹´ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã—ãŸã€‚")
            continue

        # --- ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ãƒ»è¨“ç·´ ---
        model = build_and_compile_model()
        early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)

        model.fit(
            train_generator,
            validation_data=val_generator,
            epochs=200,
            callbacks=[early_stop],
            verbose=2
        )

        # --- è©•ä¾¡ ---
        print(f"\nâ–¶ ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ï¼ˆ{test_year}ï¼‰")
        predictions = model.predict(val_generator)
        Y1_pred = predictions[0]
        Y1_true = np.concatenate([val_generator[i][1]['Yhat1'] for i in range(len(val_generator))])

        rmse = np.sqrt(mean_squared_error(Y1_true, Y1_pred))
        corr, _ = pearsonr(Y1_true.flatten(), Y1_pred.flatten())

        results[test_year] = {'rmse': rmse, 'corr': corr}
        print(f"  - RMSE: {rmse:.4f}")
        print(f"  - ç›¸é–¢ä¿‚æ•°: {corr:.4f}")

        np.savez(f"prediction_{test_year}.npz", Y1_true=Y1_true, Y1_pred=Y1_pred)
        print(f"ğŸ“ äºˆæ¸¬çµæœã‚’ prediction_{test_year}.npz ã«ä¿å­˜ã—ã¾ã—ãŸ")

    # --- ç·åˆçµæœè¡¨ç¤º ---
    print("\n=== ç·åˆè©•ä¾¡çµæœ ===")
    for year in sorted(results.keys()):
        print(f"  {year}: RMSE={results[year]['rmse']:.4f}, ç›¸é–¢ä¿‚æ•°={results[year]['corr']:.4f}")

# 4. ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œãƒ–ãƒ­ãƒƒã‚¯
if __name__ == "__main__":
    print(" å¤§è±†åé‡äºˆæ¸¬ãƒ¢ãƒ‡ãƒ« - ç·åˆå®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    run_training_and_evaluation()
    print("\n å…¨å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")